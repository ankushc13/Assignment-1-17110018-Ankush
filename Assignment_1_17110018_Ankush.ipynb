{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1-17110018-Ankush.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF5Aj_DjXkz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.probability import FreqDist\n",
        "#nltk.download('wordnet')\n",
        "# nltk.download('punkt')\n",
        "data = pd.read_csv(\"tweets-dataset.csv\")\n",
        "sent=data['Sentence'].values.tolist()\n",
        "new_data=''\n",
        "for i in sent:\n",
        "    new_data=new_data+i+' '\n",
        "\n",
        "new_data=re.sub(r'http/(?:url|URL)|#[/A-Za-z0-9_]+|@[A-Za-z0-9_]+|https?://(?:[A-Za-z0-9]|[./])+','',new_data)\n",
        "new_data=re.sub(r'twitter.com(?:[A-Za-z0-9]|[./])+','',new_data)\n",
        "new_data=re.sub(r'[^A-Za-z ]+',' ',new_data)\n",
        "# with open(\"data.txt\",\"w\") as f:\n",
        "#  \tf.write(new_data)\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jqXu5KiXqbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######Q1#######Q1############Q1\n",
        "\n",
        "tokens=len(word_tokenize(new_data))\n",
        "types=len(set(word_tokenize(new_data)))\n",
        "print(\"Tokens: {}\".format(tokens),\"Types: {}\".format(types),\"TTR: {}\".format(types/tokens),sep=\"\\n\")\n",
        "\n",
        "###############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIE9-0TpXsfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######Q2##########Q2###########Q2\n",
        "#word=random.choices(list(set(word_tokenize(new_data))),k=36908)\n",
        "\n",
        "\n",
        "synonyms = defaultdict(int)\n",
        "word=['tank','disappoints','be','Sales','way','plagiarism','centralisation','Surveillance','communication','procrastinating','responsibilities']\n",
        "#for extraction of meaning i used WORDNET\n",
        "for i in word:\n",
        "\tlword=[]\n",
        "\tfor syn in wordnet.synsets(i):\n",
        "\t\tfor l in syn.lemmas():\n",
        "\t\t\tlword.append(l.name())\n",
        "\tif len(set(lword))!=0:\n",
        "\t\tsynonyms[i]=len(set(lword))\n",
        "\n",
        "# n_word=[i for i in synonyms.keys()]\n",
        "# length=[]\n",
        "# new_dict=defaultdict(int)\n",
        "# for i in synonyms.keys():\n",
        "# \tif len(i) not in length:\n",
        "# \t\tlength.append(len(i))\n",
        "# \t\tnew_dict[i]=synonyms[i]\n",
        "\n",
        "length=[math.sqrt(1/len(i)) for i in word]\n",
        "meaning=[synonyms[i] for i in word]\n",
        "# print(synonyms,meaning,length,end=\"\\n\")\n",
        "plt.xlabel('1/sqrt(length)')\n",
        "plt.ylabel('meaning')\n",
        "plt.scatter(length,meaning, c=(0,0,0))\n",
        "plt.show() \n",
        "\n",
        "###########################################################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxfdh5PXX2wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########Q3############Q3#############Q3###########\n",
        "\n",
        "i = 0\n",
        "heap_dict=defaultdict(int)\n",
        "token=word_tokenize(new_data)\n",
        "words = set()\n",
        "for word in token:\n",
        "    words.add(word)\n",
        "    i += 1\n",
        "    heap_dict[i]=len(words)\n",
        "N=[int(i) for i in heap_dict.keys()]\n",
        "V=[int(heap_dict[i]) for i in heap_dict.keys()]\n",
        "plt.xlabel('Size of corpus N')\n",
        "plt.ylabel('Vocabulary size |V|')\n",
        "plt.plot(N,V)\n",
        "plt.show()\n",
        "\n",
        "###################################################"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
